Name,Year,Link,Summary,Resources,Authors
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,Oct 2018,https://arxiv.org/abs/1810.04805,BERT ,,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
Cross-lingual Language Model Pretraining,Jan 2019,https://arxiv.org/abs/1901.07291,XLM paper,https://github.com/facebookresearch/XLM,"Guillaume Lample, Alexis Conneau"
RoBERTa: A Robustly Optimized BERT Pretraining Approach,Jul 2019,https://arxiv.org/abs/1907.11692,RoBERTa,,"Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov"
Well-Read Students Learn Better: On the Importance of Pre-training Compact Models,Aug 2019,https://arxiv.org/abs/1908.08962,"TLDR: Pretrain students before distilling for better downstream NLU performance
They construct multiple variants of bert by changing the widths and depths - eg let W=768, D=12 for bert-base, they construct bert models with W=128,256... and D=2,3,..
Setup - Teacher:Bert-base trained on some task, pretrain the smaller berts, Use teacher to label large unlabelled corpus (distillation). 
They find that pretraining the student before performing distillation is very helpful. They also find that deeper models are generally better students",,"Iulia Turc, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
ALBERT: A Lite BERT for Self-supervised Learning of Language Representations,Sep 2019,https://arxiv.org/abs/1909.11942,ALBERT,https://github.com/google-research/ALBERT,"Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut"
StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding,Sep 2019,https://arxiv.org/abs/1908.045777,StructBERT,,"Wei Wang, Bin Bi, Ming Yan, Chen Wu, Zuyi Bao, Jiangnan Xia, Liwei Peng, Luo Si"
ERNIE: Enhanced Language Representation with Informative Entities,ACL 2019,https://arxiv.org/abs/1905.07129,ERNIE,https://github.com/PaddlePaddle/ERNIE,"Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, Qun Liu"
Unicoder: A Universal Language Encoder by Pre-training with Multiple Cross-lingual Tasks,ACL 2019,https://aclanthology.org/D19-1252/,Unicoder,,"Haoyang Huang, Yaobo Liang, Nan Duan, Ming Gong, Linjun Shou, Daxin Jiang, Ming Zhou"
Unified Language Model Pre-training for Natural Language Understanding and Generation,NIPS 2019,https://arxiv.org/abs/1905.03197,UniLM,https://github.com/microsoft/unilm,"Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, Hsiao-Wuen Hon"
Rethinking embedding coupling in pre-trained language models,Oct 2020,https://arxiv.org/abs/2010.12821,RemBERT,,"Hyung Won Chung, Thibault Févry, Henry Tsai, Melvin Johnson, Sebastian Ruder"
Unsupervised Cross-lingual Representation Learning at Scale,ACL 2020,https://arxiv.org/abs/1911.02116,XLMR,https://github.com/facebookresearch/XLM,"Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov"
"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",ACL 2020,https://aclanthology.org/2020.acl-main.703/,BART,,"Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer"
Multilingual Denoising Pre-training for Neural Machine Translation,TACL 2020,https://aclanthology.org/2020.tacl-1.47/,mBART,,"Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, Luke Zettlemoyer"
Language Models are Unsupervised Multitask Learners,,https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf,GPT2,,Alec Radford * 1 Jeffrey Wu * 1 Rewon Child 1 David Luan 1 Dario Amodei ** 1 Ilya Sutskever **
XLNet: Generalized Autoregressive Pretraining for Language Understanding,Jan 2020,https://arxiv.org/abs/1906.08237,XLNet,https://github.com/zihangdai/xlnet,"Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le"
ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators,ICLR 2020,https://arxiv.org/abs/2003.10555,Electra,https://github.com/google-research/electra,"Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning"
Language Models are Few-Shot Learners,Jul 2020,https://arxiv.org/abs/2005.14165,GPT3,,"Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei"
Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,JMLR 2020,https://arxiv.org/abs/1910.10683,T5,https://github.com/google-research/text-to-text-transfer-transformer,"Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu"
Cross-Lingual Natural Language Generation via Pre-Training,AAAI 2020,https://arxiv.org/abs/1909.10481,XNLG,,"Zewen Chi, Li Dong, Furu Wei, Wenhui Wang, Xian-Ling Mao, Heyan Huang"
mT5: A massively multilingual pre-trained text-to-text transformer,Mar 2021,https://arxiv.org/abs/2010.11934,mT5,https://github.com/google-research/multilingual-t5,"Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin Raffel"
MuRIL: Multilingual Representations for Indian Languages,Mar 2021,https://arxiv.org/abs/2103.10730,MuRIL,,"Simran Khanuja, Diksha Bansal, Sarvesh Mehtani, Savya Khosla, Atreyee Dey, Balaji Gopalan, Dilip Kumar Margam, Pooja Aggarwal, Rajiv Teja Nagipogu, Shachi Dave, Shruti Gupta, Subhash Chandra Bose Gali, Vish Subramanian, Partha Talukdar"
DeBERTa: Decoding-enhanced BERT with Disentangled Attention,Oct 2021,https://arxiv.org/abs/2006.03654,,https://github.com/microsoft/DeBERTa,"Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen"
DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing,Dec 2021,https://arxiv.org/abs/2111.09543,,https://github.com/microsoft/DeBERTa,"Pengcheng He, Jianfeng Gao, Weizhu Chen"
VECO: Variable and Flexible Cross-lingual Pre-training for Language Understanding and Generation,ACL 2021,https://arxiv.org/abs/2010.16046,VECO,,"Fuli Luo, Wei Wang, Jiahao Liu, Yijia Liu, Bin Bi, Songfang Huang, Fei Huang, Luo Si"
Improving Pretrained Cross-Lingual Language Models via Self-Labeled Word Alignment,ACL 2021,https://aclanthology.org/2021.acl-long.265/,XLM-Align,https://github.com/CZWin32768/XLM-Align,"Zewen Chi, Li Dong, Bo Zheng, Shaohan Huang, Xian-Ling Mao, Heyan Huang, Furu Wei"
English Intermediate-Task Training Improves Zero-Shot Cross-Lingual Transfer Too,AACL 2021,https://aclanthology.org/2020.aacl-main.56/,X-STILTS,,"Jason Phang, Iacer Calixto, Phu Mon Htut, Yada Pruksachatkun, Haokun Liu, Clara Vania, Katharina Kann, Samuel R. Bowman"
InfoXLM: An Information-Theoretic Framework for Cross-Lingual Language Model Pre-Training,NAACL 2021,https://aclanthology.org/2021.naacl-main.280/,InfoXLM,https://github.com/microsoft/unilm/tree/master/infoxlmm,"Zewen Chi, Li Dong, Furu Wei, Nan Yang, Saksham Singhal, Wenhui Wang, Xia Song, Xian-Ling Mao, Heyan Huang, Ming Zhou"
Explicit Alignment Objectives for Multilingual Bidirectional Encoders,NAACL 2021,https://arxiv.org/abs/2010.07972,AMBER,https://github.com/junjiehu/amberr,"Junjie Hu, Melvin Johnson, Orhan Firat, Aditya Siddhant, Graham Neubig"
ERNIE-M: Enhanced Multilingual Representation by Aligning Cross-lingual Semantics with Monolingual Corpora,EMNLP 2021,https://arxiv.org/abs/2012.15674,ERNIE-M,https://github.com/PaddlePaddle/ERNIE/tree/repro/ernie-m,"Xuan Ouyang, Shuohuan Wang, Chao Pang, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang"
mT6: Multilingual Pretrained Text-to-Text Transformer with Translation Pairs,EMNLP 2021,https://aclanthology.org/2021.emnlp-main.125/,mT6,,"Zewen Chi, Li Dong, Shuming Ma, Shaohan Huang, Saksham Singhal, Xian-Ling Mao, Heyan Huang, Xia Song, Furu Wei"
On Learning Universal Representations Across Languages,ICLR 2021,https://arxiv.org/abs/2007.15960,HiCTL,,"Xiangpeng Wei, Rongxiang Weng, Yue Hu, Luxi Xing, Heng Yu, Weihua Luo"
XLM-E: Cross-lingual Language Model Pre-training via ELECTRA,ACL 2022,https://arxiv.org/abs/2106.16138,XLM-E,,"Zewen Chi, Shaohan Huang, Li Dong, Shuming Ma, Bo Zheng, Saksham Singhal, Payal Bajaj, Xia Song, Xian-Ling Mao, Heyan Huang, Furu Wei"